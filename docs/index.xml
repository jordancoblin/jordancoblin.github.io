<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Meanderings</title>
    <link>/</link>
    <description>Recent content on AI Meanderings</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Sep 2024 15:51:45 -0600</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ML Foundations: Understanding the Math Behind Backpropagation</title>
      <link>/posts/mnist_nn/</link>
      <pubDate>Wed, 18 Sep 2024 15:51:45 -0600</pubDate>
      <guid>/posts/mnist_nn/</guid>
      <description>&lt;p&gt;The past decade has marked a heyday for neural network, driving innovations from deep learning advancements to the rise of transformer models that power tools like ChatGPT, Claude, and other large language models. Recently, Geoffrey Hinton was even &lt;a href=&#34;https://www.utoronto.ca/news/geoffrey-hinton-wins-nobel-prize#:~:text=Geoffrey%20Hinton%2C%20a%20University%20Professor,2024%20Nobel%20Prize%20in%20Physics&#34;&gt;awarded the Nobel Prize in Physics&lt;/a&gt; for his pioneering contributions to neural networks - a testament to the profound impact of these models on both AI and society.&lt;/p&gt;&#xA;&lt;p&gt;While a variety of powerful libraries, such as PyTorch, TensorFlow, and JAX, have simplified the process of training and deploying neural networks, developing an understanding of their underlying principles remains invaluable. In this post, Iâ€™ll guide you through the mathematical underpinnings of backpropagation, a key algorithm for training neural networks, and demonstrate how to implement it from scratch using Python with NumPy. Weâ€™ll apply this knowledge to train a simple fully connected neural network for classifying images in the &lt;a href=&#34;https://www.kaggle.com/datasets/hojjatk/mnist-dataset&#34;&gt;MNIST dataset&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>/about/</link>
      <pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate>
      <guid>/about/</guid>
      <description>&lt;figure class=&#34;img-avatar&#34;&gt;&lt;img src=&#34;/images/avatar.jpeg&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;Hey there ðŸ‘‹ My name is &lt;strong&gt;Jordan&lt;/strong&gt;, and I am a machine learning engineer who enjoys exploring and experimenting with new ideas. This blog is a space to share some of those explorations.&lt;/p&gt;&#xA;&lt;p&gt;I completed my M.Sc. with Adam White in the &lt;a href=&#34;http://rlai.ualberta.ca/&#34;&gt;RLAI Lab&lt;/a&gt; at the University of Alberta, where my &lt;a href=&#34;https://drive.google.com/file/d/1Vz8l086xKTiqASWE-_ufIc8G-RMQ24sa/view?usp=sharing&#34;&gt;thesis&lt;/a&gt; was on applying reinforcement learning to a real-world water treatment plant. I&amp;rsquo;ve also worked on computer vision for agricultural robots at &lt;a href=&#34;https://upsiderobotics.com/&#34;&gt;Upside Robotics&lt;/a&gt;, blockchain collectibles at &lt;a href=&#34;https://www.dapperlabs.com/&#34;&gt;Dapper Labs&lt;/a&gt;, and conversational analytics at &lt;a href=&#34;https://www.tableau.com/&#34;&gt;Tableau&lt;/a&gt;. In my spare time, I enjoy training in the art of folding clothes while people are still in them, also commonly referred to as Brazilian Jiu Jitsu.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>/archives/</link>
      <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
      <guid>/archives/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>/homepage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/homepage/</guid>
      <description></description>
    </item>
  </channel>
</rss>
