<!doctype html>



































<html
  class="not-ready lg:text-base"
  style="--bg: #faf8f1"
  lang="en-us"
  dir="ltr"
>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>From Zero to Predicting Digits: Coding a Neural Network by Hand - AI Meanderings</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="Welcome to my very first post of the blog! I wanted to take some time to brush up on ML foundations now that I&rsquo;m in between jobs, and what better place to start than popular computer vision tasks? Knowing myself, I have a habit of not always completing projects that I begin, so my hope is that treating blog posts as completion artifacts for these projects will be a useful forcing function for seeing things through." />
  <meta name="author" content="AI Meanderings" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="//localhost:1313/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="//localhost:1313/theme.png" />

  
  
  
  
  

  
  
  <link rel="preload" as="image" href="//localhost:1313/github.svg" />
  
  <link rel="preload" as="image" href="//localhost:1313/linkedin.svg" />
  
  

  
  
  <script
    defer
    src="//localhost:1313/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  
  

  
  <link
    rel="icon"
    href="//localhost:1313/favicon.ico"
  />
  <link
    rel="apple-touch-icon"
    href="//localhost:1313/apple-touch-icon.png"
  />

  
  <meta name="generator" content="Hugo 0.134.2">

  
  
  

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/contrib/auto-render.min.js"
      onload="renderMathInElement(document.body);"></script>

  <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
  </script>
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-[--w] px-8 lg:justify-center">
  <div class="relative z-50 ltr:mr-auto rtl:ml-auto flex items-center">
    <a class="-translate-y-[1px] text-2xl font-medium" href="//localhost:1313/"
      >AI Meanderings</a
    >
    <div
      class="btn-dark text-[0] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 ltr:-mr-8 rtl:-ml-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    
    <nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse">
      
      <a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/about/"
        >About</a
      >
      
    </nav>
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 rtl:space-x-reverse dark:invert ltr:lg:ml-14 rtl:lg:mr-14 lg:mt-0 lg:items-center"
    >
      
      <a
        class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/jordancoblin"
        target="_blank"
        rel="me"
      >
        github
      </a>
      
      <a
        class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./linkedin.svg)"
        href="https://linkedin.com/in/jordan-coblin-59237597"
        target="_blank"
        rel="me"
      >
        linkedin
      </a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-[--w] px-8 pb-16 pt-14 dark:prose-invert"
    >
      

<article>
  <header class="mb-14">
    <h1 class="!my-0 pb-2.5">From Zero to Predicting Digits: Coding a Neural Network by Hand</h1>

    
    <div class="text-xs antialiased opacity-60">
      
      <time>Sep 18, 2024</time>
      
      
      
      
    </div>
    
  </header>

  <section><p>Welcome to my very first post of the blog! I wanted to take some time to brush up on ML foundations now that I&rsquo;m in between jobs, and what better place to start than popular computer vision tasks? Knowing myself, I have a habit of not always completing projects that I begin, so my hope is that treating blog posts as completion artifacts for these projects will be a useful forcing function for seeing things through.</p>
<p>Into the meaty content. In this post, I will walk through the implementation of a simple fully-connected neural network to tackle image classification on the <a href="https://www.kaggle.com/datasets/hojjatk/mnist-dataset">MNIST dataset</a>, which contains 70,000 28x28 pixel images of handwritten digits. I will implement backpropagation and stochastic gradient descent from scratch using <code>numpy</code> and provide high-level derivations and intuition for computing weight updates of each of the neurons, but I&rsquo;ll try not to get overly academic with it. This was a fun and surprisingly challenge exercise, and it made me even more thankful that mature automatic differentiation libraries like <code>pytorch</code> exist - I imagine that manually computing gradients for a 30+ layer ResNet would entail a special kind of masochism.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="mnist-digit-classification">MNIST Digit Classification</h2>
<p>Let&rsquo;s begin by giving a brief overview of the MNIST dataset and image classification task. As mentioned, MNIST is comprised of 28x28 pixel images of handwritten digits, and is divided into 60,000 training images and 10,000 test images.</p>
<p>Each image has a corresponding label which is a real number in the range $[0, 9]$. Naturally, the task will be to design an algorithm which is able to correctly classify as many images in our dataset (or more precisely, our test set) correctly.</p>
<h4 id="sample-images-with-corresponding-labels">Sample images with corresponding labels:</h4>
<p><img src="images/mnist_sample_with_labels.png" alt="MNIST sample"></p>
<h2 id="neural-network-overview">Neural Network Overview</h2>
<p>Let&rsquo;s first begin by sa</p>
<p>Our neural network will consist of a single hidden layer, where each node in the hidden layer applies an activation function to a weighted sum of the inputs. The choice of activation function is crucial, as it introduces non-linearity to the model, enabling it to learn complex patterns.</p>
<p>TODO: define mathematically.</p>
<p>In this example, we’ll implement a fully connected (FC) network using Python and NumPy. We initialize random weights for each layer and choose the sigmoid function as the activation function. Feel free to swap it out with others like ReLU or tanh, depending on the task.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid_derivative</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> sigmoid(x) <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> sigmoid(x))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">FCNetwork</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Single hidden layer network&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, input_dim, hidden_dim, output_dim, activation<span style="color:#f92672">=</span>sigmoid):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>w1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(input_dim, hidden_dim) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.01</span> <span style="color:#75715e"># d x h</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>w2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(hidden_dim, output_dim) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.01</span> <span style="color:#75715e"># h x 10</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>b1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">1</span>, hidden_dim)) <span style="color:#75715e"># 1 x h</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>b2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">1</span>, output_dim)) <span style="color:#75715e"># 1 x 10</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>activation <span style="color:#f92672">=</span> activation
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, X):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>z1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X, self<span style="color:#f92672">.</span>w1) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b1
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>a1 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(self<span style="color:#f92672">.</span>z1)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>z2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(self<span style="color:#f92672">.</span>a1, self<span style="color:#f92672">.</span>w2) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b2
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>a2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>softmax(self<span style="color:#f92672">.</span>z2)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>a2
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">softmax</span>(self, z):
</span></span><span style="display:flex;"><span>        exp_z <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>exp(z <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>max(z, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> exp_z <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sum(exp_z, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p>Here, our network&rsquo;s forward propagation step computes the output probabilities using the softmax function. We’ll use a sigmoid function in the hidden layer for non-linearity, and softmax at the output to compute probabilities for each class (since this is a classification problem).</p>
<h2 id="defining-the-loss-function">Defining the Loss Function</h2>
<p>For our classification task, we’ll use the <strong>cross-entropy loss</strong>, which is a common choice for multi-class classification problems. It measures the difference between the predicted probability distribution and the true distribution (one-hot encoded labels for MNIST).</p>
<h3 id="python-code-for-cross-entropy-loss">Python Code for Cross-Entropy Loss:</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cross_entropy_loss</span>(y, y_hat):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Small epsilon added to avoid log(0)</span>
</span></span><span style="display:flex;"><span>    epsilon <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-12</span>
</span></span><span style="display:flex;"><span>    y_hat <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>clip(y_hat, epsilon, <span style="color:#ae81ff">1.</span> <span style="color:#f92672">-</span> epsilon)  <span style="color:#75715e"># Ensure y_hat is within (0, 1) to prevent log(0)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Average over the batch</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>sum(y <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(y_hat)) <span style="color:#f92672">/</span> y<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span></code></pre></div><p>In this function, we first clip the predicted values y_hat to avoid undefined values from log(0) and then compute the average loss over the batch of examples.</p>
<h2 id="implementing-backpropagation">Implementing Backpropagation</h2>
<p>Next up, we implement <strong>backpropagation</strong>, which is the algorithm that allows the model to update its weights based on the gradient of the loss function with respect to each parameter. This is done using the chain rule of calculus to propagate the error from the output layer back to the input layer.</p>
<p>TODO: derive update rules</p>
<h3 id="python-code-for-backpropagation">Python Code for Backpropagation:</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backprop</span>(X, y, model, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Forward pass</span>
</span></span><span style="display:flex;"><span>    y_hat <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>forward(X)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Compute the error at the output layer</span>
</span></span><span style="display:flex;"><span>    dz2 <span style="color:#f92672">=</span> y_hat <span style="color:#f92672">-</span> y  <span style="color:#75715e"># (batch_size, 10)</span>
</span></span><span style="display:flex;"><span>    dw2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(model<span style="color:#f92672">.</span>a1<span style="color:#f92672">.</span>T, dz2) <span style="color:#f92672">/</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    db2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(dz2, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#f92672">/</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Compute the error at the hidden layer</span>
</span></span><span style="display:flex;"><span>    dz1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(dz2, model<span style="color:#f92672">.</span>w2<span style="color:#f92672">.</span>T) <span style="color:#f92672">*</span> sigmoid_derivative(model<span style="color:#f92672">.</span>z1)
</span></span><span style="display:flex;"><span>    dw1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X<span style="color:#f92672">.</span>T, dz1) <span style="color:#f92672">/</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    db1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(dz1, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#f92672">/</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Update weights and biases</span>
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>w2 <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> dw2
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>b2 <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> db2
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>w1 <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> dw1
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>b1 <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> db1
</span></span></code></pre></div><p>The backpropagation algorithm updates the weights (w1 and w2) and biases (b1 and b2) by computing the gradients of the loss with respect to each parameter. These gradients are used to adjust the parameters in the direction that reduces the loss, as governed by the learning rate.</p>
<h2 id="evaluating-performance">Evaluating Performance</h2>
<p>After training the model, we want to evaluate how well it generalizes to unseen data (our test set). The accuracy metric is a simple yet effective measure, especially for classification tasks like MNIST.</p>
<h3 id="python-code-for-accuracy">Python Code for Accuracy:</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy</span>(y_true, y_pred):
</span></span><span style="display:flex;"><span>    y_true_labels <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(y_true, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    y_pred_labels <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(y_pred, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>mean(y_true_labels <span style="color:#f92672">==</span> y_pred_labels)
</span></span></code></pre></div><p>Here, we convert the one-hot encoded labels and predictions into their respective class indices using argmax, and then compute the percentage of correctly predicted examples.</p>
<h2 id="training-the-model">Training the Model</h2>
<p>We can now tie everything together in a training loop. The model will iterate over the training data, compute the loss, backpropagate the errors, and update its parameters.</p>
<h3 id="python-code-for-training-loop">Python Code for Training Loop:</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(epochs):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> batch_idx, (X_batch, y_batch) <span style="color:#f92672">in</span> enumerate(train_loader):
</span></span><span style="display:flex;"><span>        X_batch <span style="color:#f92672">=</span> X_batch<span style="color:#f92672">.</span>view(X_batch<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>numpy()  <span style="color:#75715e"># Flatten the input images</span>
</span></span><span style="display:flex;"><span>        y_batch_onehot <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>eye(<span style="color:#ae81ff">10</span>)[y_batch<span style="color:#f92672">.</span>numpy()]  <span style="color:#75715e"># Convert labels to one-hot encoding</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Forward and Backpropagation</span>
</span></span><span style="display:flex;"><span>        backprop(X_batch, y_batch_onehot, model, learning_rate)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Test performance on test set</span>
</span></span><span style="display:flex;"><span>    test_X <span style="color:#f92672">=</span> test_loader<span style="color:#f92672">.</span>dataset<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">28</span><span style="color:#f92672">*</span><span style="color:#ae81ff">28</span>)<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>    test_y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>eye(<span style="color:#ae81ff">10</span>)[test_loader<span style="color:#f92672">.</span>dataset<span style="color:#f92672">.</span>targets<span style="color:#f92672">.</span>numpy()]
</span></span><span style="display:flex;"><span>    test_predictions <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>forward(test_X)
</span></span><span style="display:flex;"><span>    test_accuracy <span style="color:#f92672">=</span> accuracy(test_y, test_predictions)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>epochs<span style="color:#e6db74">}</span><span style="color:#e6db74"> - Test Accuracy: </span><span style="color:#e6db74">{</span>test_accuracy<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>This loop trains the model for a set number of epochs, where each epoch processes the entire dataset. After each epoch, we compute the accuracy on the test dataset.</p>
<h2 id="debugging">Debugging</h2>
<p>Training a neural network from scratch can often result in a few hiccups along the way, including issues like vanishing gradients, slow convergence, or poor generalization. A few debugging tips:</p>
<p>Check the learning rate: If the model is not improving, the learning rate may be too high or too low.
Inspect gradients: If the weights are not updating properly, inspect the gradients and make sure they are neither too large nor vanishingly small.
Try different activations: Sigmoid can suffer from saturation in deep networks. Experiment with ReLU or Leaky ReLU if needed.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this post, we’ve implemented a fully connected neural network from scratch using NumPy, trained it using stochastic gradient descent and backpropagation, and tested it on the MNIST dataset. This foundational understanding will be useful as we move to more advanced architectures.</p>
<p>Next, we’ll take on the challenge of implementing a <strong>convolutional neural network (CNN)</strong> to tackle a more complex dataset, the CIFAR-10, where image recognition becomes more nuanced.</p>
<p>Stay tuned!</p>
</section>

  
  

  
  
  
  
  

  
  
  <div class="mt-24" id="disqus_thread"></div>
  <script>
    const disqusShortname = 'https-jordancoblin-github-io';
    const script = document.createElement('script');
    script.src = 'https://' + disqusShortname + '.disqus.com/embed.js';
    script.setAttribute('data-timestamp', +new Date());
    document.head.appendChild(script);
  </script>
  

  
  

  


  
</article>


    </main>

    <footer
  class="mx-auto flex h-[4.5rem] max-w-[--w] items-center px-8 text-xs uppercase tracking-wider opacity-60"
>
  <div class="mr-auto">
  
    &copy; 2024
    <a class="link" href="//localhost:1313/">AI Meanderings</a>
  
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >powered by hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >hugo-paper</a
  >
</footer>

  </body>
</html>
