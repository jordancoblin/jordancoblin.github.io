<!doctype html>



































<html
  class="not-ready lg:text-base"
  style="--bg: #faf8f1"
  lang="en-us"
  dir="ltr"
>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>Backpropagation by Hand: Gradient Derivations for MNIST Classification - AI Meanderings</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="Welcome to my very first post of the blog! I wanted to take some time to brush up on ML foundations and what better way to learn (or re-learn) technical topics than to write up one&rsquo;s findings? I&rsquo;m also hoping that treating these blog posts as final artifacts will be a useful forcing function for actually completing the projects.
Into the meaty content. In this post, I will walk through the implementation of a simple fully-connected neural network to tackle image classification on the MNIST dataset. I will implement backpropagation and stochastic gradient descent from scratch using numpy and provide high-level derivations and intuition for computing weight updates of each of the neurons, but I&rsquo;ll try not to get overly academic with it. This was a fun and surprisingly challenging exercise, and it made me even more thankful that mature automatic differentiation libraries like pytorch exist - I imagine that manually computing gradients for a 30&#43; layer ResNet would entail a special kind of masochism." />
  <meta name="author" content="AI Meanderings" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="//localhost:1313/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="//localhost:1313/theme.png" />

  
  
  
  
  

  
  
  <link rel="preload" as="image" href="//localhost:1313/github.svg" />
  
  <link rel="preload" as="image" href="//localhost:1313/linkedin.svg" />
  
  

  
  
  <script
    defer
    src="//localhost:1313/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  
  

  
  <link
    rel="icon"
    href="//localhost:1313/favicon.ico"
  />
  <link
    rel="apple-touch-icon"
    href="//localhost:1313/apple-touch-icon.png"
  />

  
  <meta name="generator" content="Hugo 0.134.2">

  
  
  

  

  
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
  </script>

  <script type="text/javascript">
  MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],  
    displayMath: [['$$', '$$'], ['\\[', '\\]']],  
    tags: 'ams',  
    processEscapes: true
  }
  };
  </script>

</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-[--w] px-8 lg:justify-center">
  <div class="relative z-50 ltr:mr-auto rtl:ml-auto flex items-center">
    <a class="-translate-y-[1px] text-2xl font-medium" href="//localhost:1313/"
      >AI Meanderings</a
    >
    <div
      class="btn-dark text-[0] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 ltr:-mr-8 rtl:-ml-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    
    <nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse">
      
      <a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/about/"
        >About</a
      >
      
    </nav>
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 rtl:space-x-reverse dark:invert ltr:lg:ml-14 rtl:lg:mr-14 lg:mt-0 lg:items-center"
    >
      
      <a
        class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/jordancoblin"
        target="_blank"
        rel="me"
      >
        github
      </a>
      
      <a
        class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./linkedin.svg)"
        href="https://linkedin.com/in/jordan-coblin-59237597"
        target="_blank"
        rel="me"
      >
        linkedin
      </a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-[--w] px-8 pb-16 pt-14 dark:prose-invert"
    >
      

<article>
  <header class="mb-14">
    <h1 class="!my-0 pb-2.5">Backpropagation by Hand: Gradient Derivations for MNIST Classification</h1>

    
    <div class="text-xs antialiased opacity-60">
      
      <time>Sep 18, 2024</time>
      
      
      
      
    </div>
    
  </header>

  <section><p>Welcome to my very first post of the blog! I wanted to take some time to brush up on ML foundations and what better way to learn (or re-learn) technical topics than to write up one&rsquo;s findings? I&rsquo;m also hoping that treating these blog posts as final artifacts will be a useful forcing function for actually completing the projects.</p>
<p>Into the meaty content. In this post, I will walk through the implementation of a simple fully-connected neural network to tackle image classification on the <a href="https://www.kaggle.com/datasets/hojjatk/mnist-dataset">MNIST dataset</a>. I will implement backpropagation and stochastic gradient descent from scratch using <code>numpy</code> and provide high-level derivations and intuition for computing weight updates of each of the neurons, but I&rsquo;ll try not to get overly academic with it. This was a fun and surprisingly challenging exercise, and it made me even more thankful that mature automatic differentiation libraries like <code>pytorch</code> exist - I imagine that manually computing gradients for a 30+ layer ResNet would entail a special kind of masochism.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="mnist-digit-classification">MNIST Digit Classification</h2>
<!-- raw HTML omitted -->
<p>Let&rsquo;s begin by laying some notational groundwork for the MNIST classification task. As usual for supervised learning problems, we consider the setting where we are provided a dataset $\mathcal{D}$ consisting of input vectors $x$ and label vectors $y$:</p>
<p>$$\mathcal{D} = \bigl\lbrace (x^{(i)}, y^{(i)}) \bigr\rbrace_{i=1}^m, $$</p>
<p>where $m$ is the number of samples in our dataset. The standard MNIST dataset consists of 60,000 training images and 10,000 test images, which we will call $\mathcal{D_{\text{train}}}$ and $\mathcal{D_{\text{test}}}$. An image can be represented as a column vector:</p>
<p>$$x^{(i)} = [x_1^{(i)}, x_2^{(i)}, &hellip;, x_{n_x}^{(i)}]^T,$$</p>
<p>where $n_x = 28 \times 28$ is the number of pixels in each image. Each image has a real-valued label $y^{(i)} \in [0, 9]$ that indicates which digit, or class, the image corresponds to. To help us perform classification, we will represent this as a one-hot encoded vector:</p>
<!-- raw HTML omitted -->
<p>$$y^{(i)} = [y_1^{(i)}, y_2^{(i)}, &hellip;, y_{n_y}^{(i)}]^T,$$ where $n_y = 10$ is the number of digits or classes to choose from and</p>
<p>$$
y_k^{(i)} = \begin{cases}
1 &amp; \text{if class } k \text{ is the correct class}, \\\
0 &amp; \text{otherwise}.
\end{cases}
$$</p>
<p>Below we can see some sample images from this dataset, along with their corresponding labels.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><img src="images/mnist_sample_with_labels.png" alt="MNIST sample"></p>
<p>Because we have multiple digits to choose from, we consider this a <strong>multi-class classification</strong> problem, where the goal is roughly to find some function $f(x)$ that is able to correctly determine the labels for as many images in our dataset (or more precisely, our test set) as possible.</p>
<h2 id="neural-network-definition">Neural Network Definition</h2>
<p>TODO: introduce batch/vector notation here</p>
<p>Most of you are probably familiar enough with neural networks that I can skip a conceptual introduction. Instead, I will move into defining the neural network as a mathematical function, so that we can work with each part for our backprop derivations.</p>
<p>Let $f(x; W)$ be the classification function (model) parameterized by weights $W$ and biases $b$; for notation simplicity, we can absorb $b$ into $W$. This classification function outputs the predicted label $\hat{y}^{(i)} = f(x^{(i)}; W) = \arg\max_c f_c(x^{(i)}; W)$, where $f_c(x^{(i)}; W)$ is the score or probability for class $c$. This function $f_c$ is what we will be modeling with our neural network.</p>
<!-- raw HTML omitted -->
<p>Neural networks may have an abritrary number of layers (hence the name <em>deep</em> learning), and we will use the notation $W^{[l]}$ and $b^{[l]}$ to denote the weights and biases for layer $l$. For our model, we will use a network with a single hidden layer of size 128. The output of this hidden layer is:</p>
<p>$$h = \sigma (W^{[1]} x^{(i)} + b^{[1]}),$$</p>
<p>where $W^{[1]} \in \mathbb{R}^{n_h \times n_x}$ is the hidden layer&rsquo;s weight matrix, $b^{[1]} \in \mathbb{R}^{n_x}$ is the bias vector, $n_h = 128$ is the hidden layer size, and $\sigma$ is the sigmoid activation function. The dimensions of each matrix and vector become quite important during implementation - shape errors tend to be where I spend much of my debugging time in the early stages of a project.</p>
<p>For classification problems where a single label is predicted, it is typical to use the softmax function to convert the final layer outputs into a probability distribution:</p>
<p>$$\text{softmax}(z) = \frac{e^{z}}{\sum_{j=1}^{C} e^{z}_{j}}.$$</p>
<p>With this, the final output of our neural network becomes:</p>
<p>$$f_c(x^{(i)}; \theta) = \text{softmax} (W^{[2]} h + b^{[2]}),$$</p>
<p>where $W^{[2]} \in \mathbb{R}^{n_y \times n_h}$ and $b^{[2]} \in \mathbb{R}^{n_y}$ are the <em>output</em> layer&rsquo;s weight matrix and bias vector, respectively.</p>
<p>Pictorally, our network looks something like this&hellip; TODO</p>
<!-- raw HTML omitted -->
<p>And here is my implementation of a fully-connected neural network (i.e. FCNetwork) in python:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">softmax</span>(z):
</span></span><span style="display:flex;"><span>    exp_z <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>exp(z <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>max(z, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))  <span style="color:#75715e"># Subtract max(z) for numerical stability</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> exp_z <span style="color:#f92672">/</span> exp_z<span style="color:#f92672">.</span>sum(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">FCNetwork</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Single hidden layer network&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, input_dim, hidden_dim, output_dim, activation<span style="color:#f92672">=</span>sigmoid):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>w1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(input_dim, hidden_dim) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">1.</span> <span style="color:#f92672">/</span> input_dim) <span style="color:#75715e"># d x h</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>w2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(hidden_dim, output_dim) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">1.</span> <span style="color:#f92672">/</span> hidden_dim) <span style="color:#75715e"># h x 10</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>b1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">1</span>, hidden_dim) <span style="color:#75715e"># 1 x h</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>b2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">1</span>, output_dim) <span style="color:#75715e"># 1 x 10</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>activation <span style="color:#f92672">=</span> activation
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, X):
</span></span><span style="display:flex;"><span>        batch_size <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        X <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>reshape((batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>        z1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X, self<span style="color:#f92672">.</span>w1) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b1
</span></span><span style="display:flex;"><span>        h <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(z1)
</span></span><span style="display:flex;"><span>        z2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(h, self<span style="color:#f92672">.</span>w2) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b2
</span></span><span style="display:flex;"><span>        f_c <span style="color:#f92672">=</span> softmax(z2)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> z1, h, z2, f_c
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, X):
</span></span><span style="display:flex;"><span>        _, _, _, f_c <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>forward(X)
</span></span><span style="display:flex;"><span>        y_hat <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(f_c, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> y_hat
</span></span></code></pre></div><p>The <code>forward</code> function returns a vector of softmax distributions $f_c$ for a batch of samples <code>X</code>, along with other variables that will be useful for backpropagation, while the <code>predict</code> function returns a vector of predicted classes $\hat{y}^{(i)}$.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="gradient-descent-with-backpropagation">Gradient Descent with Backpropagation</h2>
<p>We now have a parameterized model that is capable of representing a variety of functions. Our goal is to find the function which provides the best fit with respect to our dataset $\mathcal{D}$. To accomplish this, we will introduce a <strong>loss function</strong> $J(W)$ as a measure of fit, and then <em>minimize</em> this function to find the optimal parameters of the model:</p>
<p>$$W_* = \arg\min_{W} J(W).$$</p>
<p>For multi-class classification problems, cross-entropy is a common loss function which measures the distance between the distribution produced by our model, and the true distribution $P(y|x)$. The cross-entropy loss for a single tuple $(x^{(i)}, y^{(i)})$ is defined as:</p>
<p>$$
\begin{equation}
\label{eq:loss}
J(W) = - \sum_{k=1}^{K} y_k^{(i)} \log \hat{y}_k^{(i)}
\end{equation}
$$</p>
<p>where $K = n_y$ is the number of classes.</p>
<p>To solve this optimization problem, we will use <strong>gradient descent</strong> with the <strong>backpropagation</strong> algorithm, which I will assume the reader has some familiarity with. At a high level, backpropagation allows us to efficiently compute the derivatives needed to perform gradient updates using the chain rule in calculus. During this process, derivatives from later layers in the network get passed back through previous layers, hence the name!</p>
<h2 id="deriving-the-backprop-learning-updates">Deriving the Backprop Learning Updates</h2>
<p>At this point, the fastest way forward would be to use an automatic differentiation library like <code>pytorch</code> to handle all the gradient computations and not muddle ourselves in all the mathematical details. But where would be the fun in that? Let&rsquo;s go ahead and derive the gradient descent updates ourselves.</p>
<p>Updating parameters $\theta$ at each iteration of gradient descent is a matter of taking a step in the direction of steepest descent in the loss function, with step size $\alpha$:</p>
<p>$$ \theta \leftarrow \theta - \alpha \nabla J(\theta).$$</p>
<p>Breaking down the gradient by each set of weights and biases in our network, we arrive at the following four equations to be solved:</p>
<p>$$
\begin{align*}
W^{[1]} &amp; \leftarrow W^{[1]} - \alpha \frac{\partial J}{\partial W^{[1]}} \\\
b^{[1]} &amp; \leftarrow b^{[1]} - \alpha \frac{\partial J}{\partial b^{[1]}} \\\
W^{[2]} &amp; \leftarrow W^{[2]} - \alpha \frac{\partial J}{\partial W^{[2]}} \\\
b^{[2]} &amp; \leftarrow b^{[2]} - \alpha \frac{\partial J}{\partial b^{[2]}}. \\\
\end{align*}
$$</p>
<p>It&rsquo;s important to remember that $W^{[l]}$ is a <em>matrix</em> and $b^{[l]}$ is a <em>vector</em>, so the result of each derivative here will be either a matrix or vector as well. The components of these derivative objects is the partial derivative with respect to <em>each individual weight</em>. That is,</p>
<p>$$
\begin{equation}
\label{eq:jacobian}
\frac{\partial J}{\partial W^{[l]}} =
\begin{bmatrix}
\frac{\partial J}{\partial W_{1,1}^{[l]}} &amp; \frac{\partial J}{\partial W_{1,2}^{[l]}} &amp; \cdots &amp; \frac{\partial J}{\partial W_{1,n_{l-1}}^{[l]}} \\\
\frac{\partial J}{\partial W_{2,1}^{[l]}} &amp; \frac{\partial J}{\partial W_{2,2}^{[l]}} &amp; \cdots &amp; \frac{\partial J}{\partial W_{2,n_{l-1}}^{[l]}} \\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\
\frac{\partial J}{\partial W_{n_l,1}^{[l]}} &amp; \frac{\partial J}{\partial W_{n_l,2}^{[l]}} &amp; \cdots &amp; \frac{\partial J}{\partial W_{n_l,n_{l-1}}^{[l]}} \\\
\end{bmatrix},
\end{equation}
$$</p>
<p>where $n_l$ and $n_{l-1}$ are the number of neurons in layers $l$ and $l-1$, respectively.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="forward-pass">Forward Pass</h3>
<p>To begin with an iteration of backpropogation, we first do a <strong>forward pass</strong>, where we pass an input $x$ through the network. During the forward pass, we compute outputs at each stage of the network, and store some which will be used later during the backward pass. We introduce the variable $z^{[l]}$, to aid us during the backward pass as well:</p>
<p>$$
\begin{align*}
z^{[1]} &amp;= W^{[1]} x + b^{[1]} \\\
h &amp;= \sigma(z^{[1]}) \\\
z^{[2]} &amp;= W^{[2]} h + b^{[2]} \\\
\hat{y} &amp;= \text{softmax}(z^{[2]}).
\end{align*}
$$</p>
<p>At this stage, it is helpful if we visualize how all of these outputs and parameters fit together. For simplicity, we&rsquo;ll consider a network with 3 neurons in the hidden layer, 2 dimensions in the output, and 2 in the input vector.</p>
<p><strong>[Placeholder: diagram of full network]</strong></p>
<h3 id="backward-pass">Backward Pass</h3>
<p>For our <strong>backward pass</strong>, we will compute the partial derivatives needed for our learning update. To accomplish this, we use the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a> to decompose the gradient into constituent parts. Let&rsquo;s start with the weights in the output layer:</p>
<p>$$
\begin{align}
\frac{\partial J}{\partial W_{j,i}^{[2]}} = \frac{\partial J}{\partial \hat{y_j}} \frac{\partial \hat{y_j}}{\partial z_j^{[2]}} \frac{\partial z_j^{[2]}}{\partial W_{j,i}^{[2]}}
\end{align}
$$</p>
<p>For the first term, we can solve simply:</p>
<p>$$
\frac{\partial J}{\partial \hat{y_j}} = \frac{\partial}{\partial \hat{y_j}} \bigl( - \sum_{k=1}^{K} y_k \log \hat{y}_k \bigr) = -\frac{y_j}{\hat{y}_j},
$$</p>
<p>by noting that the derivative is zero for each term in the sum, save for the case where $k=j$.</p>
<p>For the third term, again we note that the derivative is zero for each term in $W^{[2]} h$, except for $W_{j,i}^{[2]} h$:</p>
<p>$$
\begin{align*}
\frac{\partial z_j^{[2]}}{\partial W_{j,i}^{[2]}} &amp;= \frac{\partial }{\partial W_{j,i}^{[2]}} \bigl( W^{[2]} h + b^{[2]} \bigr) \\\
&amp;= \frac{\partial }{\partial W_{j,i}^{[2]}} W_{j,i}^{[2]} h \\\
&amp;= h
\end{align*}
$$</p>
<p>For the second term, things are slightly trickier. Notice in diagram [TODO: reference],</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>&hellip; For the first layer&hellip; Notice that the two terms were the same that we computed</p>
<h3 id="python-code-for-backpropagation">Python Code for Backpropagation:</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backprop</span>(X, y, model, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Forward pass</span>
</span></span><span style="display:flex;"><span>    y_hat <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>forward(X)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Compute the error at the output layer</span>
</span></span><span style="display:flex;"><span>    dz2 <span style="color:#f92672">=</span> y_hat <span style="color:#f92672">-</span> y  <span style="color:#75715e"># (batch_size, 10)</span>
</span></span><span style="display:flex;"><span>    dw2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(model<span style="color:#f92672">.</span>a1<span style="color:#f92672">.</span>T, dz2) <span style="color:#f92672">/</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    db2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(dz2, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#f92672">/</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Compute the error at the hidden layer</span>
</span></span><span style="display:flex;"><span>    dz1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(dz2, model<span style="color:#f92672">.</span>w2<span style="color:#f92672">.</span>T) <span style="color:#f92672">*</span> sigmoid_derivative(model<span style="color:#f92672">.</span>z1)
</span></span><span style="display:flex;"><span>    dw1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X<span style="color:#f92672">.</span>T, dz1) <span style="color:#f92672">/</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    db1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(dz1, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#f92672">/</span> X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Update weights and biases</span>
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>w2 <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> dw2
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>b2 <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> db2
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>w1 <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> dw1
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>b1 <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> db1
</span></span></code></pre></div><p>The backpropagation algorithm updates the weights (w1 and w2) and biases (b1 and b2) by computing the gradients of the loss with respect to each parameter. These gradients are used to adjust the parameters in the direction that reduces the loss, as governed by the learning rate.</p>
<h2 id="evaluating-performance">Evaluating Performance</h2>
<p>After training the model, we want to evaluate how well it generalizes to unseen data (our test set). The accuracy metric is a simple yet effective measure, especially for classification tasks like MNIST.</p>
<h3 id="python-code-for-accuracy">Python Code for Accuracy:</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accuracy</span>(y_true, y_pred):
</span></span><span style="display:flex;"><span>    y_true_labels <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(y_true, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    y_pred_labels <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(y_pred, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>mean(y_true_labels <span style="color:#f92672">==</span> y_pred_labels)
</span></span></code></pre></div><p>Here, we convert the one-hot encoded labels and predictions into their respective class indices using argmax, and then compute the percentage of correctly predicted examples.</p>
<h2 id="training-the-model">Training the Model</h2>
<p>We can now tie everything together in a training loop. The model will iterate over the training data, compute the loss, backpropagate the errors, and update its parameters.</p>
<h3 id="python-code-for-training-loop">Python Code for Training Loop:</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>learning_rate <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(epochs):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> batch_idx, (X_batch, y_batch) <span style="color:#f92672">in</span> enumerate(train_loader):
</span></span><span style="display:flex;"><span>        X_batch <span style="color:#f92672">=</span> X_batch<span style="color:#f92672">.</span>view(X_batch<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>numpy()  <span style="color:#75715e"># Flatten the input images</span>
</span></span><span style="display:flex;"><span>        y_batch_onehot <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>eye(<span style="color:#ae81ff">10</span>)[y_batch<span style="color:#f92672">.</span>numpy()]  <span style="color:#75715e"># Convert labels to one-hot encoding</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Forward and Backpropagation</span>
</span></span><span style="display:flex;"><span>        backprop(X_batch, y_batch_onehot, model, learning_rate)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Test performance on test set</span>
</span></span><span style="display:flex;"><span>    test_X <span style="color:#f92672">=</span> test_loader<span style="color:#f92672">.</span>dataset<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">28</span><span style="color:#f92672">*</span><span style="color:#ae81ff">28</span>)<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>    test_y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>eye(<span style="color:#ae81ff">10</span>)[test_loader<span style="color:#f92672">.</span>dataset<span style="color:#f92672">.</span>targets<span style="color:#f92672">.</span>numpy()]
</span></span><span style="display:flex;"><span>    test_predictions <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>forward(test_X)
</span></span><span style="display:flex;"><span>    test_accuracy <span style="color:#f92672">=</span> accuracy(test_y, test_predictions)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>epochs<span style="color:#e6db74">}</span><span style="color:#e6db74"> - Test Accuracy: </span><span style="color:#e6db74">{</span>test_accuracy<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>This loop trains the model for a set number of epochs, where each epoch processes the entire dataset. After each epoch, we compute the accuracy on the test dataset.</p>
<h2 id="debugging">Debugging</h2>
<p>Training a neural network from scratch can often result in a few hiccups along the way, including issues like vanishing gradients, slow convergence, or poor generalization. A few debugging tips:</p>
<p>Check the learning rate: If the model is not improving, the learning rate may be too high or too low.
Inspect gradients: If the weights are not updating properly, inspect the gradients and make sure they are neither too large nor vanishingly small.
Try different activations: Sigmoid can suffer from saturation in deep networks. Experiment with ReLU or Leaky ReLU if needed.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this post, we’ve implemented a fully connected neural network from scratch using NumPy, trained it using stochastic gradient descent and backpropagation, and tested it on the MNIST dataset. This foundational understanding will be useful as we move to more advanced architectures.</p>
<p>Next, we’ll take on the challenge of implementing a <strong>convolutional neural network (CNN)</strong> to tackle a more complex dataset, the CIFAR-10, where image recognition becomes more nuanced.</p>
<p>Stay tuned!</p>
</section>

  
  

  
  
  
  
  

  
  
  <div class="mt-24" id="disqus_thread"></div>
  <script>
    const disqusShortname = 'https-jordancoblin-github-io';
    const script = document.createElement('script');
    script.src = 'https://' + disqusShortname + '.disqus.com/embed.js';
    script.setAttribute('data-timestamp', +new Date());
    document.head.appendChild(script);
  </script>
  

  
  

  


  
</article>


    </main>

    <footer
  class="mx-auto flex h-[4.5rem] max-w-[--w] items-center px-8 text-xs uppercase tracking-wider opacity-60"
>
  <div class="mr-auto">
  
    &copy; 2024
    <a class="link" href="//localhost:1313/">AI Meanderings</a>
  
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >powered by hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >hugo-paper</a
  >
</footer>

  </body>
</html>
