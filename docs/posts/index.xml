<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on AI Meanderings</title>
    <link>/posts/</link>
    <description>Recent content in Posts on AI Meanderings</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Nov 2025 22:10:23 -0700</lastBuildDate>
    <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Flu Season</title>
      <link>/posts/flu_szn/</link>
      <pubDate>Thu, 27 Nov 2025 22:10:23 -0700</pubDate>
      <guid>/posts/flu_szn/</guid>
      <description>&lt;p&gt;Every other month is flu season for my immune system, it appears. I had gone two good, productive years with minimal sickness - which I attribute primarily to being able to sleep without an alarm during grad school. Now, this has to be the fifth or sixth time this year. Absolute circus of an immune system, if you ask me.&lt;/p&gt;&#xA;&lt;p&gt;Let’s go over the symptoms, as an exercise in bodily awareness. In order of most salient, we can begin with the eyes. A general irritation of the eyeballs, with eyelids seemingly carrying the full weight of my unfullfilled dreams. In turn, with sore eyes, the mind yearns for rest - I suppose this is the function of the eye situation, the body’s way of telling you to get the f*ck to sleep and stop moving around so much. I should listen to this signal more obediently.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ML Foundations: Understanding the Math Behind Backpropagation</title>
      <link>/posts/understanding-the-math-behind-backpropagation/</link>
      <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
      <guid>/posts/understanding-the-math-behind-backpropagation/</guid>
      <description>&lt;p&gt;The past decade has marked a heyday for neural networks, driving innovations from deep learning advancements to the rise of transformer models that power tools like ChatGPT, Claude, and other large language models. Recently, Geoffrey Hinton was even &lt;a href=&#34;https://www.utoronto.ca/news/geoffrey-hinton-wins-nobel-prize#:~:text=Geoffrey%20Hinton%2C%20a%20University%20Professor,2024%20Nobel%20Prize%20in%20Physics&#34;&gt;awarded the Nobel Prize in Physics&lt;/a&gt; for his pioneering contributions to neural networks - a testament to the profound impact of these models on both AI and society.&lt;/p&gt;&#xA;&lt;p&gt;While a variety of powerful libraries, such as PyTorch, TensorFlow, and JAX, have simplified the process of training and deploying neural networks, developing an understanding of their underlying principles remains invaluable. In this post, I’ll guide you through the mathematical underpinnings of backpropagation, a key algorithm for training neural networks, and demonstrate how to implement it from scratch using Python with NumPy. We’ll apply this knowledge to train a simple fully connected neural network for classifying images in the &lt;a href=&#34;https://www.kaggle.com/datasets/hojjatk/mnist-dataset&#34;&gt;MNIST dataset&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
