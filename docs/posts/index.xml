<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on AI Meanderings</title>
    <link>/posts/</link>
    <description>Recent content in Posts on AI Meanderings</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Sep 2024 15:51:45 -0600</lastBuildDate>
    <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ML Foundations: Implementing Backpropagation from Scratch</title>
      <link>/posts/mnist_nn/</link>
      <pubDate>Wed, 18 Sep 2024 15:51:45 -0600</pubDate>
      <guid>/posts/mnist_nn/</guid>
      <description>&lt;p&gt;The last decade has seen a heyday for neural networks, driving innovations from deep learning advancements to the rise of transformer models that power tools like ChatGPT, Claude, and other large language models. Several weeks ago, Geoffrey Hinton was even &lt;a href=&#34;https://www.utoronto.ca/news/geoffrey-hinton-wins-nobel-prize#:~:text=Geoffrey%20Hinton%2C%20a%20University%20Professor,2024%20Nobel%20Prize%20in%20Physics&#34;&gt;awarded the Nobel Prize in Physics&lt;/a&gt; for his pioneering contributions to neural networks - a testament to the profound impact of these models on both AI and society.&lt;/p&gt;&#xA;&lt;p&gt;While a variety of powerful libraries, such as PyTorch, TensorFlow, and JAX, have simplified the process of training and deploying neural networks, a deep understanding of their underlying principles remains invaluable. In this post, I’ll guide you through implementing a fully connected neural network to classify images in the &lt;a href=&#34;https://www.kaggle.com/datasets/hojjatk/mnist-dataset&#34;&gt;MNIST dataset&lt;/a&gt;. I&amp;rsquo;ll cover derivations and intuition for the gradient computations in backpropagation and implement everything from scratch using NumPy. Let’s dive in!&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
